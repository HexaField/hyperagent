# StreamingLLM sidecar configuration
# Copy this file to .env.sidecar and adjust values for your environment.

STREAMING_LLM_MODEL=llama3.2:latest            # Model identifier or HF path consumed by StreamingLLMEngine
STREAMING_LLM_ENABLE=1                         # Toggle (1/0) to enable streaming-specific optimizations
STREAMING_LLM_START_SIZE=4                     # Number of tokens preserved from the beginning of the conversation
STREAMING_LLM_RECENT_SIZE=2048                 # Number of most recent tokens retained in conversation history
STREAMING_LLM_AGENTS_DIR=.agents               # Filesystem directory where markdown-backed agents are stored
STREAMING_LLM_MAX_NEW_TOKENS=512               # Upper bound on tokens generated per completion request
STREAMING_LLM_OLLAMA_URL=http://127.0.0.1:11434 # Base URL for the Ollama daemon when proxying local models
STREAMING_LLM_HOST=0.0.0.0                     # Host bound by uvicorn (set to 0.0.0.0 when running in Docker)
STREAMING_LLM_PORT=8000                        # Port used when launching uvicorn backend.server:app
STREAMING_LLM_LOG_DIR=./data/logs              # Directory where append-only JSONL logs are written
STREAMING_LLM_SUMMARY_DIR=./data/summaries     # Directory where markdown summaries are persisted
# The docker/workflow-runner/streaming-llm.compose.yml service automatically mounts ./data and loads this file.
